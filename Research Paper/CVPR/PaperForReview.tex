% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{varwidth}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}



% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Astronomical Image colorization and up-scaling with Generative Adversarial Networks}

\author{Shreyas Kalvankar\\
K. K. Wagh Institute of Engineering Education and Research\\
K. K. Wagh Institute of Engineering Education and Research, Nashik, India\\
{\tt\small shreyas.kalvankar@kkwagh.edu.in}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Hrushikesh Pandit\\
{\tt\small hrushikesh.pandit@kkwagh.edu.in}
\and
Pranav Parwate\\
{\tt\small pranav.parwate@kkwagh.edu.in}
\and
Atharva Patil\\
{\tt\small atharva.patil@kkwagh.edu.in}
\and
Snehal Kamalapur\\
{\tt\small smkamalapur@kkwagh.edu.in}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Automatic colorization of images without human intervention has been a subject of interest in the machine learning community for a brief period of time. Assigning color to an image is a highly ill-posed problem because of its innate nature of possessing very high degrees of freedom; given an image, there is often no single color-combination that is correct. Besides colorization, another problem in reconstruction of images is Single Image Super Resolution, which aims at transforming low resolution images to a higher resolution. This research aims to provide an automated approach for the problem by focusing on a very specific domain of images, namely astronomical images, and process them using Generative Adversarial Networks (GANs). We explore the usage of various models in two different color spaces, RGB and L*a*b. We use transferred learning owing to a small data set, using pre-trained ResNet-18 as a backbone, i.e. encoder for the U-net and fine-tune it further. The model produces visually appealing images which hallucinate high resolution, colorized data in these results which does not exist in the original image. We present our results by evaluating the GANs quantitatively using distance metrics such as L1 distance and L2 distance in each of the color spaces across all channels to provide a comparative analysis. We use Fr√©chet inception distance (FID) to compare the distribution of the generated images with the distribution of the real image to assess the model's performance.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Colorization of gray scale images with an automated algorithm has been under much research within the machine learning and computer vision communities. Beyond being fascinating from an aesthetic and artificial intelligence perspective, such capability has broad practical applications. It is an area of research that has untapped potential in applications: from black and white photo reconstruction, image augmentation, image enhancement to video restoration for improved interpretability. \\
\hspace*{0.167 in}Image downscaling is an innately lossy process. The principal objective of super resolution imaging is to reconstruct a low resolution image into a high resolution one based on a set of low-resolution images to rectify the limitations that existed while the procurement of the original low-resolution images, insuring better visualization and recognition for scientific and non-scientific purposes. A particularly good up-scaling algorithm will always have some data loss when producing high frequency image due to a downscale-upscale function. Ultimately, even the best up-scaling algorithms are unable to effectively reconstruct non-existing data. Conventional methods rely on low-information and a smooth interpolation between known pixels. These methods can effectively be treated as a convolution with a kernel which encodes no information about the original image. Generative Adversarial Networks (GANs) can be used to hallucinate high frequency data in a super scaled image that does not exist in the smaller image. Even after increasing the resolution, they fail to achieve the desired clarity in the task. By using the above mentioned method, not a perfect reconstruction can be obtained albeit instead a rather plausible guess can be made at what the lost data might be, constrained to reality by a loss function which penalizes deviations from the ground truth image.\\
    \begin{figure}[!htb]
    	\centering
    	\begin{subfigure}[b]{0.3\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figures/samples_main}
    		\caption{Ground Truth}
    		\label{fig: main_color_samples}
    	\end{subfigure}
    		\hspace{0.1 in}
    	\begin{subfigure}[b]{0.3\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figures/samples_full-trained}
    		\caption{ResNet18 Full trained L*a*b}
    		\label{fig: resnet18_full}
    	\end{subfigure}
    	\caption{Results of fine-tuned ResNet18 in L*a*b colorspace compared with ground truth images}
    \end{figure}
\hspace*{0.167 in}As noted in \cite{Gao2019astronomical}, a huge number of raw images are unprocessed and unnoticed in the Hubble Legacy Archives. These raw images, typically black and white, low-resolution, are unfit to be shared with the world. It takes huge amounts of hours to process them. This processing is necessary because it's difficult for astronomers to distinguish objects from the raw images and is further made necessary owing to noise from other bodies in the universe, changing optical characteristics in the system and random \& synthetic noise from the sensors in the telescope. Furthermore, for the process of highlighting small features that ordinarily wouldn't be able to be picked out against noise of the image, we need colorization. The processing of the images is so time consuming that the images are rarely seen by human eyes. Not only is new data being continuously produced by Hubble Telescope, but new telescopes are soon to come online. This goes to show that the problem will likely get worse. A simplification of image processing by using artificial image colorization and super-resolution can be done in an automated fashion to make it easier for astronomers to visually identify and analyze objects in Hubble dataset.
\begin{figure}[!htb]
    	\centering
    	\begin{subfigure}[b]{0.15\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figures/original_sample}
    		\caption{Original image}
    		\label{fig: original_sample}
    	\end{subfigure}
    	\begin{subfigure}[b]{0.15\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figures/GAN_output_sample}
    		\caption{GAN output}
    		\label{fig: gan_output_sample}
    	\end{subfigure}
    	\caption{\cref{fig: original_sample} is the original image and \cref{fig: gan_output_sample} is the image produced by our GAN}
    	\label{fig: sample_comp}
\end{figure}
\section{Literature Review}
\subsection{Image Colorization}
\subsubsection{Hint Based Colorization}
\hspace*{0.167 in}Levin \etal\cite{levin2004colorization} proposed using colorization hints from the user in a quadratic cost function which imposed that neighboring pixels in space-time with similar intensities should have similar colours. This was a simple but effective method but only had hints provided in form of imprecise colored scribbles on the grayscale input. With no additional information about the image, the method was able to efficiently generate high quality colorized output. Huang \etal\cite{huang2005edge} addressed and solved the color bleeding issue using adaptive edge detection. Yatziv \& Sapiro\cite{yatziv2006chrominance} used luminescence based weighting in hints to boost efficiency. Qu \etal\cite{qu2006manga} extended the original cost function to apply color continuity over similar textures along with intensities.
\subsubsection{Deep Colorization}
\hspace*{0.167 in}The recent advances in Convolutional Neural Networks have made them a de facto standard for solving image classification problems. CNNs are peculiar in their ability to learn and differentiate colors, patterns and shapes within an image and their ability to associate them with different classes.\\
\hspace*{0.167 in}Cheng \etal\cite{cheng2016deep} proposed a per pixel training for neural networks using DAISY \cite{tola2008descriptor}, and semantic \cite{long2015semantic} features to predict the chrominance value for each pixel, that used bilateral filtering to smooth out accidental image artifacts. With a large enough dataset, this method proved to be superior to the example based techniques even with a simple Euclidean loss function against the ground truth values. Finally, Dahl\cite{dahl2016automatic} successfully implemented a system to automatically colorize black \& white images using several ImageNet-trained layers from VGG-16 \cite{simonyan2015deep} and integrating them with auto-encoders that contained residual connections. These residual connections merged the outputs produced by the encoding VGG16 layers and the decoding portion of the network in the later stages. He \etal\cite{he2015deep} showed that deeper neural networks can be trained by reformulating layers to learn residual function with reference to layer inputs. Using this \textit{Residual Connections}, He \etal\cite{he2015deep} created the \textit{ResNets} that went as deep as 152 layers and won the 2015 ImageNet Challenge.
\subsubsection{Generative Adversarial Networks}
\hspace*{0.167 in}Goodfellow \etal\cite{goodfellow2014generative} introduced the adversarial framework that provides an approach to training a neural network which uses the generative distribution of $p_g(x)$ over the input data $x$.\\
\hspace*{0.167 in}Since it's inception in 2015, many extended works of GAN have been proposed over years including DCGAN \cite{radford2016unsupervised}, Conditional-GAN \cite{mirza2014conditional}, iGAN \cite{zhu2018generative}, Pix2Pix \cite{isola2018imagetoimage}.\\
\hspace*{0.167 in}Radford \etal\cite{radford2016unsupervised} applied the adversarial framework for training convolutional neural networks as generative models for images, demonstrating the viability of \textit{deep convolutional generative adversarial networks}.\\
\hspace*{0.167 in}DCGAN is the standard architecture to generate images from random noise. Instead of generating images from random noise, Conditional-GAN \cite{mirza2014conditional} uses a condition to generate output image. For \eg a grayscale image is the condition for colorization of image. Pix2Pix \cite{isola2018imagetoimage} is a Conditional-GAN with images as the conditions. The network can learn a mapping from input image to output image and also learn a separate loss function to train this mapping. Pix2Pix is considered to be the state of the art architecture for image-image translation problems like colorization.
\subsection{Image Upscaling}
\subsubsection{The interpolation based SR image approach}
\hspace*{0.167 in} The interpolation-based SR approach constructs a high resolution image by casting all the low resolution images to the reference image and then combining all the information available from every image available.
The method consists of the following three stages
(i) the registration stage; aligns the low-resolution input images,
(ii) the interpolation stage; produces a higher-resolution image, and
(iii) the deblurring stage; enhances the reconstructed high-resolution image produced. However, as each low resolution image adds a few new details before deblurring them, this method cannot be used if only a single reference image is available.\\
\hspace*{0.167 in} Most known Bayesian-based SR approaches are maximum likelihood (ML) estimation approach  and maximum a posterior (MAP) estimation approach. While Tom \& Katsaggelos \cite{Brian1996ML} proposed the first ML estimation based SR approach with the aim to find the ML estimation of high resolution image, some proposed a MAP estimation approach. MAP SR tries to takes into consideration the prior image model to reflect the expectation of the unknown high resolution image.
\subsubsection{Super Resolution - Generative Adversarial Networks (SR-GAN)}
\hspace*{0.167 in}Ledig \etal\cite{ledig2017photorealistic} introduced SRGAN in 2017, which used a SRResNet to upscale images with an upscaling factor of 4x. SRGAN is currently the state of the art on public benchmark datasets.
\section{Methodology}
     \begin{figure*}[!htb]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/gan_visualization.pdf}
        \caption{Approach for image colorization using conditional GANs. A sample grayscale image is input to the generator and the generated image along with the ground truth is fed to the discriminator. \textit{p} is the probability that the discriminator thinks the two images are similar.}
        \label{fig:gan_visualization}
    \end{figure*}
    \subsection{Data collection}
    \hspace*{0.167 in}The data is scraped off the Hubble Legacy Archive. The scrapper tool %\footnote{Fork: https://github.com/obi-wan-shinobi/hla-scraper}
    which was used, courtesy of Peh \& Marshland \cite{Gao2019astronomical}\footnote{Original repository: https://github.com/KMarshland/hla-scraper}, scraped off hundreds of thousands of colorized images the archive has available. The Hubble Legacy archive is slow and produces grainy images with a lot of noise and majority are unprocessed. A filter for M101 (Messier 101) galaxy rendered more than 80 thousand images with a 1 degree difference between consecutive right ascension. The data acquired was large and with no efficient way to clean it without human investment. We needed high resolution and well colored images for traning the SRGAN. We scraped the Hubble Heritage project instead. The Hubble Heritage project releases the highest-quality astronomical images which are stitched together, colorized and processed to eliminate noise. Heritage then selects the best of these for public release. However, there were only $\sim150$ of these images that were actually useful. We scraped images from the main Hubble website so as to increase the amount of data we had. This provided another $\sim1000$ images approx. Limited by our computational resources, we used images in dimensions of $256\times 256$ pixels with RGB color channels.
    \subsection{Image Color Space}
    \hspace*{0.167 in}An RGB image is essentially a rank 3 tensor of height, width and color. The data is represented in RGB color space which has 3 numbers for every pixel indicating the amount of \textit{Red, Green, and Blue} values the pixel has.
    \begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
    	\includegraphics[width=\textwidth]{figures/rgb_colorspace}
    	\caption{Reg, Green and Blue channels of an image (Main image by \href{https://unsplash.com/@aznbokchoy}{Lucas Benjamin})}
    	\label{rgb_colorspace}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
    	\includegraphics[width=\textwidth]{figures/Lab_colorspace}
    	\caption{Lightness, *a and *b channels of the L*a*b colorspace}
    	\label{lab_colorspace}
    \end{subfigure}
    \caption{Color spaces}
    \end{figure}
    In \textbf{L*a*b} color space, we have three numbers for each pixel but these numbers have different meanings.L, the first channel, has the \textbf{Lightness} of each pixel encoded and when we visualize this channel it appears as a black and white image. 
    The \textbf{*a and *b} channels encode in them exactly how much \textbf{green-red} and \textbf{yellow-blue} each pixel is, respectively.
    In \cite{guadarrama2017pixcolor,isola2018imagetoimage} the use of L*a*b color space is proposed instead of RGB to train the models. Intuitively, to the train a model for colorization, a grayscale image should be fed to it and we hope that it colors it. In the L*a*b colorspace, the model is fed the L channel, which is essentially a grayscale image, and we perform computations to predict the other two channels (*a and *b). After the predictions, we concatenate the channels and get a colorful image. In case of RGB, there is a need to explicitly convert the three channels down to 1 to make it a grayscale image and then feed it to the network hoping that it predicts three numbers per pixel. This is an unstable task due to sheer increase in volume of combinations from two numbers to three. We train models using both color spaces and compare their performance.
    \subsection{Mathematical Model}
    \subsubsection{Generative Adversarial Networks}
     A generative network, $G$, is supposed to learn the underlying distribution of a latent space, Y. Instead of visually assessing the quality of network outputs and judge how we can adapt the network to produce convincing results, we incorporate automatic tweaking during training by introducing a discriminative network $D$. The network D takes in both the fabricated outputs generated by G and real inputs from the underlying distribution Y. The network produces a probability of the image belonging to the real or fabricated space.
     Let $x \in X$ be a low resolution or a grayscale image and $y \in Y$ be it's underlying distribution from the latent space Y. Generator $G$ takes in input $x$ and produces an output $\hat{y}$. We define the mapping $x \rightarrow \hat{y}$ in the following manner:
      \begin{equation}
          \footnotesize{G(x) = \hat{y}}
      \end{equation}
     The discriminative network D is fed the fabricated mapping $x \rightarrow \hat{y}$ and the underlying distribution of $x$ i.e. $y \in Y$. The network D then produces a result that is a probability distribution of the input space indicating the class of the image that it thinks the input belongs to. We define this as:
     \begin{equation}
         \footnotesize{D\big(G(x),y\big) = p}
     \end{equation}
     where $p \in (0,1)$ is the probability that the image is fabricated or real.
    With conditional GAN, both generator and discriminator are conditioning on the input $x$. Let the generator be parameterized by $\theta_g$ and the discriminator be parameterized by $\theta_d$. The minimax objective function can be defined as:
    \begin{multline}
    \min_{\theta_g}\max_{\theta_d}\Big[\mathbb{E}_{x,y\sim p_{\tiny{data}}} \log D_{\theta_d}(x,y) \\+ E_{{x\sim p_{\tiny{data}}}} \log(1 - D_{\theta_d}(x, G_{\theta_g}(x))\Big]
        \normalsize
    \end{multline}
    
    Where, $G_{\theta_{g}}$ is the output of the generator and $D_{\theta_d}$ is the output of the discriminator. We do not introduce any noise in our generator to keep things simple for the time being. Also, we consider $L1$ difference between input $x$ and output $y$ in generator. On each iteration, the discriminator would maximize $\theta_d$ according to the above expression and generator would minimize $\theta_g$ in the following way:
        \begin{equation}
            \footnotesize{\min_{\theta_g}\Big[-\log(D_{\theta_d}(x,G_{\theta_g}(x)))+\lambda \Vert G_{\theta_g}(x) - y \Vert_1 \Big]}
    	 \end{equation}
 \subsection{Transferred learning and model tweaking}
    \hspace*{0.167 in}A general solution is proposed in \cite{isola2018imagetoimage} for many image-to-image translation tasks. We propose a similar methodology with a few minor tweaks, so as to significantly reduce the amount of training data needed and minimize the training time to achieve similar results. Initially, we propose the use of a U-net for the generator with a pre-trained ResNet-18 network as the \textit{encoder} having half of it's layers clipped off so as to get the intermediate abstractions. It is shown in \cite{ledig2017photorealistic}, that, training the generator separately in a supervised, deterministic manner generalizes the mapping from the input space to outputs. The idea solves the problem of \textit{"The blind leading the blind"} that is persistent in most image-to-image translation tasks to date. \\
    \hspace*{0.167 in}The generator is pre-trained independently using the imagenet weights over the COCO dataset. Unlike adversarial training, this phase was supervised with \textit{mean absolute error} or the \textit{L1 Norm} as the loss from the target image. Though trained deterministically,the problem of rectifying incorrect predictions still persists due to constraints over the convergence of loss metric. To combat this, the trained generator was re-trained in an adversarial fashion for further generalization. We hypothesize that re-training in an adversarial fashion will further rectify the subtle color differences that \textit{mae} couldn't solve.\\
    \hspace*{0.167 in}A pre-trained ResNet-50 with imagenet weights was used as the discriminator with last few layers clipped off. The discriminative network used is something called a "Patch Discriminator" proposed in \cite{isola2018imagetoimage}. In a \textit{vanilla} discriminator, proposed in \cite{goodfellow2014generative}, the network produces a scalar output, representing the probability of the data $x$ belonging to the input distribution and not the generator distribution $p_g$. Isola \etal\cite{isola2018imagetoimage} proposed a modification to the discriminative network so as to produce, instead of a single scalar, a vector of probabilities representing different localities of the input distribution (image) $x$. For instance, a discriminator producing a $50 \times 50$ vector represents the probabilities every receptive field that is covered by the output vector. Thus, we can localize the corrections in the image that the generator should make.\\
    \hspace*{0.167 in}Finally, the trained generator and trained discriminator are fine-tuned to fit on our data which is rather small in size compared to the previous datasets. These networks are trained in an adversarial fashion using the conditional GAN objective function \cite{isola2018imagetoimage} with some noise introduced as the L1 norm of generated image tensor to the target image tensor. The reason being minimization of generator loss using an adversarial component while trying to minimize the Manhattan distance between the generator output and target vector space.
\subsection{Experimental Setup}
    \hspace*{0.167 in}We use a mini-batch gradient descent with a batch size of 10, 16 and 32 for different iteration with Adam that has $\beta_1 = 0.5$ and $\beta_2 = 0.999$ as momentum. The generator and discriminator have a learning rate of $2e-4$ which remains constant throughout the training. We train the model with different epochs ranging between 20,50 and 100, saving the best model weights determined by L1 norm between the output of the generator and the target image. We use early stopping with a patience value of 10. The image size is $256\times256$. The implementation uses Python, numpy, Tensorflow and tf-keras. It takes about 24 hours to complete training over the COCO dataset and about 12 to 13 hours to fit the model on given astronomical data on a NVIDIA Tesla K80 GPU.
\section{Results and Discussions}
    \hspace*{0.167 in}In the following section, we compare the results of the implemented architectures and evaluate their performance. The evaluation is done qualitatively as well as quantitatively by comparing the performance of each model using L1 and L2 distance between the predictions and targets. Though unreliable, this method provides us with a somewhat decent ground to perform a comparative study and evaluate the reliability of such metrics on GAN evaluation compared to qualitative, visual evaluation. The performance of the GANs is also measured by using Fr√©chet inception distance (FID) as an attempt to quantify how close the produced images are to the original ones.\\
    \hspace*{0.167 in}To evaluate the model performance by virtue of convergence of the objective function, we plot the generator loss throughout the training along with the discriminator.\\
    \begin{figure}[!htb]
    	\centering
    	\begin{subfigure}[b]{0.2\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figures/resnet18_generator_loss}
    		\caption{ResNet18 U-net Generator}
    		\label{fig: resnet_loss}
    	\end{subfigure}
    		\hspace{0.1 in}
    	\begin{subfigure}[b]{0.2\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figures/resnet18_discriminator_loss}
    		\caption{ResNet50 discriminator loss}
    		\label{fig: resnet50_disc_loss}
    	\end{subfigure}
    	\caption{Curve plots for ResNet18 U-net \& ResNet50 discriminator}
    	\label{fig: resnet18_curves}
    \end{figure}
    \Cref{fig: resnet_loss} and \cref{fig: resnet50_disc_loss} shows that the generator converges with a little instability. The discriminator on the other hand oscillates because of the convergence that the generator shows. GAN losses are pretty non-intuitive but we draw a few conclusions from the graphs. This pattern of oscillation and convergence repeats when networks are trained in an adversarial fashion. It is observed that the generator with a pre-trained ResNet-18 for its backbone converges decently in the beginning but later shows spikes when nearing the end of training loop. The convergence doesn't signify whether the model is predicting expected results. The loss just converges to the minimum value that the cost function descends to, permitted by the \textit{lr}. It would normally mean that the GAN has found some optimum point in the vector space that is at the lowest potential and can't decrease any further, meaning the GAN has learned enough. Due to the sheer no. of dimensions, owing to the high amount of trainable variables, such combinations, where the function converges, can be high in volume. Thus, these numbers don't provide any better understanding of the bias or variance the model is facing. We also discover that even if the loss hasn't converged well it doesn't necessarily mean that the model hasn't learned anything. On visual inspection, the generated results show similar distribution to the ground truth, even with high generator losses. This might be due to presence of a content loss parameter in the adversarial loss function which is also minimizing the L1 norm between the predictions and target.\\
    \hspace*{0.167 in}The discriminator shows an increase in the objective loss in the initial epochs, settles down in the later phase around a point, converging to some permanent number and oscillating around it. We assume this point to be a point of stability between the two networks as the networks are in a constant adversarial battle, meaning if one performs better, the other is bound to perform worse.
    \subsection{Image Colorization}
    \hspace*{0.167 in}We present a comparative study of the following models: Basic U-net generator with a residual VGG16 discriminator, hereafter referred to as Basic U-net. A modified U-net with pre-trained ResNet18 as it's backbone. We evaluate the model by training it in RGB colorspace to predict 3 \textit{nos.} for every pixel and in L*a*b colorspace where the model predicts the a and b channel alone. We then further fine-tune this model to the task of colorizing astronomical images.
    \begin{figure}[!htb]
    \centering
    	\includegraphics[width=0.27\textwidth]{figures/comparison_color}
    	\caption{Results of the colorization study.From top left, (1) the ground truth and (2) input image. The next images belong to the output of following, serially: Basic U-net, ResNet18 in RGB, ResNet18 fine-tuned in L*a*b, ResNet18 pre-trained U-net in L*a*b}
    	\label{color_results}
    \end{figure}
    \Cref{color_results} shows that on a particular example, the Basic U-net performs better at predicting results that closely map to the ground truth even if the fine-tuned ResNet-18 shows better results for larger set of inputs. It can also be observed that the Basic U-net architecture does a decent job of faking the sharpness in the original image and that makes the image appear more realistic as compared to rather blurry outputs from the other networks. \\
    \hspace*{0.167 in}We also observe that the model trained in the RGB color space performs poorly at predicting values of all color channels and that results in channel bias (blue in this case). This causes the output, though reconstructed quite accurately, to have a varied color pattern with high emphasis on one color channel. This might be caused by the deep layers which result in diminishing gradients for certain colors over time, causing the model to be strongly biased towards the blue color in this case. An increase in the volume of training data and some random pixel shuffling in forward propagation might solve this problem. \\
    \hspace*{0.167 in}The pre-trained ResNet18 U-net performs decently with the weights gathered by training it over the COCO dataset. The model still lacks the specific coloring intuition in astronomical images and plainly colors specific parts of the images in light colors, leaving majority of the image unaltered. This causes the images to retain grayness.
    \Cref{fig: comparisons} shows how the other models perform in different color spaces. We observe that the Basic U-net model performs good at predicting the outputs but suffers at predicting the brightness level of pixels. The model seems to be overfitting on the dataset and suffers a high variance on output as demonstrated in \Cref{fig: u-net_overfit}.
    \begin{figure}[!htb]
    	\centering
    	\begin{subfigure}[b]{0.15\textwidth}
    		\centering
    		\includegraphics[width=0.9\textwidth]{figures/overfit_1}
    		\caption{Sample 1}
    		\label{fig: overfit1}
    	\end{subfigure}
    		\hspace{0.1 in}
    	\begin{subfigure}[b]{0.15\textwidth}
    		\centering
    		\includegraphics[width=0.9\textwidth]{figures/overfit_2}
    		\caption{Sample 2}
    		\label{fig: overfit2}
    	\end{subfigure}
    	\caption{Performance of Basic U-net over samples different from the dataset. Left: prediction, right: ground truth}
    	\label{fig: u-net_overfit}
    \end{figure}
    
    \Cref{fig: main_color_samples} shows the ground truth images and \Cref{fig: resnet18_full} shows the predictions of the ResNet18 full trained model in the L*a*b color space. This model seems to perform best, visually. 
    \begin{figure}[!htb]
    	\centering
    	\begin{subfigure}[t]{0.3\textwidth}
    		\centering
    		\includegraphics[width=0.7\textwidth]{figures/samples_pix2pix}
    		\caption{Basic U-net}
    		\label{fig: u-net_samples}
    	\end{subfigure}
    	\begin{subfigure}[t]{0.3\textwidth}
    		\centering
    		\includegraphics[width=0.7\textwidth]{figures/samples_pre-trained}
    		\caption{ResNet18 pre-trained L*a*b}
    		\label{fig: resnet18_pre}
    	\end{subfigure}
    	\begin{subfigure}[t]{0.3\textwidth}
    		\centering
    		\includegraphics[width=0.7\textwidth]{figures/samples_coco}
    		\caption{ResNet18 pre-trained RGB}
    		\label{fig: resnet18_rgb}
    	\end{subfigure}
    	\caption{Results of colorization models (from top to bottom): Basic U-net, ResNet18 U-net pre-trained in L*a*b color space, ResNet18 U-net pre-trained in RGB color space}
    	\label{fig: comparisons}
    \end{figure}
    To quantitatively estimate the model performance, we measure the L1 and L2 distance between the predictions and the ground truth images in both RGB as well as L*a*b colorspace. 
    \input{tables/colorization_results}
    \Cref{color_results} shows that the RGB colorspace isn't ideal for the task of colorization. In terms of the L1 distance, the best performance is achieved by ResNet18 U-net with pre-trained weights. This goes to prove the unreliability of distance metrics in performance evaluation. The model, though quantitatively, performs better than the finetuned model but in reality, fails to produce visually convincing images. The L2 distance metric shows how the fine-tuned model might, in reality, be fitting slightly better over the data to predict correct color combination as its output. We further investigate the per-channel predictions by the model.
    \input{tables/channel_wise_color}
    \Cref{tab:channel_wise_results} shows the RGB channel averages of the outputs produced by the fine tuned model. It can be observed that the feature embeddings produced by the model in *a, *b colorspaces maps to the green channel with the least error. This could indicate that the model performance is poor on images with high content of red-blue colors. 
    \input{tables/lab_color_results} 
In \Cref{tab:lab_color_results}, L1 distance shows the performance of fine-tuned model is better in *a but poor in *b compared to the pre-trained model. The L2 metric rules out the possibility of the fine-tuned model performing better than the pre-trained model. In reality, the fine-tuned model is orders of magnitude better at predicting output abstractions with the L channel as its input, thus contradicting the quantitative results.
    \input{tables/RGBchannel_wise_color}
    We can observe the comparative results of ResNet18 pre-trained model and the fine-tuned model. It can be concluded from the observations that the fine-tuned model performs better at predicting the Red color channel but suffers in Green and Blue.
    \input{tables/FID_color}
    We use Fr√©chet inception distance (FID) to evaluate the distribution of generated images with the distribution of real images. \Cref{tab:colorization_FID} helps us see that the fine-tuned U-net with ResNet-18 as it's backbone achieves the least FID score. This shows that while this model is very adept at hallucinating images it's still not able to predict accurate color values. 
    \subsection{Image Super Resolution}
    \hspace*{0.167 in}We implement the basic SR-GAN proposed by Ledig \etal \cite{ledig2017photorealistic} and train it to improve super-resolution task. We compare the trained model with pre-trained SRGAN model, EDSR-GAN proposed in \cite{lim2017enhanced} and WDSR-GAN proposed in \cite{yu2018wide}.
    \begin{figure}[!htb]
    \centering
    	\includegraphics[width=0.27\textwidth]{figures/comparison_upscale}
    	\caption{Results of the super-resolving models. The first two images in the top row are the input image and ground truth respectively. The next images belong to the output of following, serially: Fine-tuned SRGAN, pre-trained SRGAN, EDSR, WDSR}
    	\label{fig: sr_results}
    \end{figure}
    \Cref{fig: sr_results} shows that the networks perform really well and its difficult to distinguish the outputs visually. Upon closer observation, the fine-tuned network seems to produces images that look slightly better than the other counter-parts. The best performing network seems to be the WDSR-GAN.
    \begin{figure}[!htb]
    	\centering
    	\begin{subfigure}[b]{0.35\textwidth}
    		\centering
    		\includegraphics[width=0.75\textwidth]{figures/inputsamples_superR}
    		\caption{Input Distribution}
    		\label{fig: sr_input_samples}
    	\end{subfigure}
    	\begin{subfigure}[b]{0.35\textwidth}
    		\centering
    		\includegraphics[width=0.75\textwidth]{figures/samples_SRGAN}
    		\caption{SRGAN fine-tuned output}
    		\label{fig: sr__outputs}
    	\end{subfigure}
    	\caption{Results of SRGAN. \cref{fig: sr_input_samples} shows the input data and \cref{fig: sr__outputs} shows the corresponding output}
    	\label{fig: sr_comparisons}
    \end{figure}
    It is evident that the model produces acceptable results on visual inspection. The main reason behind this might, again, be the random pixel shuffling between every upscaling pass. 
    As opposed to colorization, super-resolution needs a quantitative estimation to determine which model performs best among the give models. 
    \input{tables/super-resolution_results}
    \Cref{tab:Super-resolution_results} shows the L1 and L2 distances of predicted results with the ground truth image. It is observed that Ledig's SRGAN, after a bit of fine-tuning performs really well in comparison to the the pre-trained version. To further improve this, Lim \etal\cite{lim2017enhanced} proposed an optimized version of SRGAN by removing the unnecessary modules in the conventional resnets and showed that these Enhanced Deep Residual Networks performed better at upscaling task. When trained in an adversarial manner, the ED-SRGAN performs better than the traditional SRGAN.\\
\hspace*{0.167 in}Yu \etal\cite{yu2018wide} further improved the idea by increasing the widening factor ($\times 2$ and $\times 4$) to ($\times 6$ and $\times 9$). This Wide Activation Deep Super Resolution network further improved the performance. When we implement this in an adversarial manner, we achieve excellent results. 
    \input{tables/FID_super-resolution}
The FID score listed in \Cref{tab:super-resolution_FID} show the lowest FID score is given by the fine-tuned SRGAN (which is ideally what we expect), but this just strengthens our previous claim that GANs are particularly good at hallucinating things rather than predicting them. 
\section{Summary and Conclusion}
    The project mainly tackles the problem of colorizing astronomical images and super-resolving them for astronomical inspection. We explore various methodologies proposed till date, efficient at colorizing and upscalng images, with have results significantly closer to the ground truth distribution. After scraping the data from Hubble Legacy Archive, Hubble Main website and Hubble Heritage project and created a filtered and clean dataset of 4700 images. We split the data and use 500 images, roughly 10\% of the data for testing purposes. To compensate for the lack of data, we exploit transferred learning with pre-trained architectures and fine-tune their abstractions over our dataset to find the most effective solution.\\
    \hspace*{0.167 in}The colorizing model explores the use of U-net architectures ranging from a basic U-net model with different color spaces to empirically confirm the superiority of L*a*b color space in image colorization. A ResNet-18  is used as backbone of the encoder and a U-net is built with it. The pre-trained network with COCO distributions in RGB colorspace shows significantly weaker results when compared to the similar network in L*a*b colorspace. The best performing model turns out to be the ResNet18 U-net which is fine-tuned over our dataset to produce results similar to the ground truth.\\
    \hspace*{0.167 in}The Super-resolution model is based on the SRGAN proposed in \cite{ledig2017photorealistic}. We use the generator weights and sample results from the training set to inspect the results. The model performs excellently with pre-trained weights. After fine-tuning the model, we train other state of the art SISR models such as EDSR \cite{lim2017enhanced} and WDSR \cite{yu2018wide}. These provide further insights into the problem and simultaneously, improve the results.\\
    \hspace*{0.167 in}While studying and improving the performance of these models, we explore performance metrics of GANs and evaluation methodologies implemented to test out conditional GANs. It is evident that the loss curves of generator and discriminator do not provide a lot of intuition about the model performance. We also discover that standard distance metrics cannot be used to evaluate GANs and quantitative methods that exist to evaluate GANs are unreliable. We prove so by contradiction of qualitative samples and quantitative measurements of the best performing architecture for colorization models. However, we observe that quantitative estimation is quite reliable for the problem of single image super-resolution and can be helped to determine which model is better suited for the task.
    
    \section{Future Scope}
    \hspace*{0.167 in}Though we obtain moderately good results, a vast amount of algorithms still remain unscratched. A more powerful model such as SE-ResNext, EfficientNet and more such state-of-the-art models can be implemented and trained over millions of images from the Imagenet. With even more hardware resources and availability of data, we can explore computationally heavy models for a better approximation. With help of an image stitching algorithm, produced images can be stitched together to generate large scale astronomical images for scientific study. Colorization can be improved by the virtue of exploring different loss functions using weighted losses to reduce loss problem for low saturation regions. We can introduce a gradient penalty for the SRGAN architecture and include the WGAN \cite{arjovsky2017wasserstein} which will stabilize the discriminator by enforcing conditions which result in a Lipschitz constant $<1$, so that it will stay within the space of 1-Lipschitz function. Progressively growing GANs \cite{karras2018progressive} can be applied so that the dimensions can be further improved with more stability and greater sharpness.
%-------------------------------------------------------------------------

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
